{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desarrollado por María Lourdes Linares Barrera y Pablo Reina Jiménez.   \n",
    "Proyecto para la asignatura Análisis de Información no Estructurada.  \n",
    "Máster en Ingeniería del Software Cloud, Datos y Gestión TI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Distinguir géneros musicales utilizando modelos de aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección nos centraremos en entrenar un modelo basado en las características obtenidas en el notebook anterior. Con estas características trataremos de identificar si cada una de las canciones pertenecen al género clásico o no clásico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import os\n",
    "import netron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Modelos de aprendizaje basados en extracción de características (*bag of songs*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. Creación del corpus de datos tabulares de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, haremos uso de torch para crear un dataset haciendo uso de las características de audio generadas anteiormente. El uso de esta estructura nos facilitará la posterior carga de datos en un DataLoader y su uso para el entrenamiento del modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, features_file, scaler=None):\n",
    "\n",
    "        df = pd.read_csv(features_file)\n",
    "\n",
    "        self.X = df.drop(['audio_file', 'label'], axis=1)\n",
    "        self.y = df['label'].values.astype(np.int64)\n",
    "\n",
    "        if scaler:\n",
    "            self.X = scaler.transform(self.X)\n",
    "        else:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.X = self.scaler.fit_transform(self.X)\n",
    "\n",
    "    def get_scaler(self):\n",
    "        return self.scaler\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.tolist()\n",
    "    \n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccmusic_train = TabularDataset('ccmusic/train/features.csv')\n",
    "train_scaler = ccmusic_train.get_scaler()\n",
    "ccmusic_train_dataloader = torch.utils.data.DataLoader(ccmusic_train, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       shuffle=True)\n",
    "\n",
    "ccmusic_validation = TabularDataset('ccmusic/validation/features.csv', train_scaler)\n",
    "ccmusic_validation_dataloader = torch.utils.data.DataLoader(ccmusic_validation, \n",
    "                                                            batch_size=BATCH_SIZE, \n",
    "                                                            shuffle=False)\n",
    "\n",
    "ccmusic_test = TabularDataset('ccmusic/test/features.csv', train_scaler)\n",
    "ccmusic_test_dataloader = torch.utils.data.DataLoader(ccmusic_test, \n",
    "                                                      batch_size=BATCH_SIZE, \n",
    "                                                      shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Definición del modelo para la clasificación de generos en base a las características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a la elección del modelo, hemos optado por un modelo simple de perceptrón multicapa, con 3 capas lineales de 128, 64 y 1 neurona respectivamente. El restulado de esta última neurona nos indicará si la predicción corresponde a música clásica o no clásica. Como función de pérdida usaremos BinaryCrossEntropy. Con está función de loss no es necesario aplicar un función de activación sigmoide en la última capa del modelo. Además, haciendo uso del conjunto de validación, hemos aplicado la técnica de early stopping con una paciencia de 5. Con esto nos aseguramos que el modelo no se sobreajuste con los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "\n",
    "        self.linear_block = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            # torch.nn.BatchNorm1d(128),\n",
    "            # torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            # torch.nn.BatchNorm1d(64),\n",
    "            torch.nn.Linear(64, num_classes if num_classes > 2 else 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_epoch(model, train_dataloader, val_dataloader, loss_fn, optimizer, device):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for inputs, targets in train_dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(inputs)\n",
    "        if isinstance(loss_fn, torch.nn.BCEWithLogitsLoss):\n",
    "            loss = loss_fn(predictions, targets.float().unsqueeze(1))\n",
    "        elif isinstance(loss_fn, torch.nn.CrossEntropyLoss):\n",
    "            loss = loss_fn(predictions, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            predictions = model(inputs)\n",
    "            if isinstance(loss_fn, torch.nn.BCEWithLogitsLoss):\n",
    "                loss = loss_fn(predictions, targets.float().unsqueeze(1))\n",
    "            elif isinstance(loss_fn, torch.nn.CrossEntropyLoss):\n",
    "                loss = loss_fn(predictions, targets)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_dataloader)\n",
    "\n",
    "    return loss.item(), val_loss\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, loss_fn, optimizer, epochs, patience=5, device='cuda'):\n",
    "\n",
    "    model.to(device)\n",
    "    print(\"Inicio del entrenamiento\")\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Época {epoch+1} \", end='')\n",
    "        loss, val_loss = train_single_epoch(model, train_dataloader, val_dataloader, loss_fn, optimizer, device)\n",
    "        print(f\"Loss: {loss}\")\n",
    "\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Deteniendo entrenamiento en la época {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    print(\"Fin del entrenamiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez entrenado el modelo, pasamos a evaluar los resultados del conjunto de test, para determinar la bondad del modelo. Debido a que el dataset se encuentra desbalanceado, calcularemos las méticas de accuracy y F1, lo que nos indicará si realmente el modelo está realizando buenas predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, num_classes, device='cuda'):\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        for inputs, target in dataloader:\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            output = model(inputs)\n",
    "            if num_classes > 2:\n",
    "                output = torch.argmax(output, dim=1)\n",
    "            else:\n",
    "                output = torch.sigmoid(output)\n",
    "                output = (output > 0.5)\n",
    "            predictions.append(output)\n",
    "            targets.append(target)\n",
    "        predictions = torch.cat(predictions, dim=0)\n",
    "        targets = torch.cat(targets, dim=0)\n",
    "        return {\n",
    "            'acc': accuracy_score(targets.cpu(), predictions.cpu()),\n",
    "            'f1': f1_score(targets.cpu(), predictions.cpu()) if num_classes == 2 \n",
    "            else f1_score(targets.cpu(), predictions.cpu(), average='micro')\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Entrenamiento e inferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez definidas las funciones necesarias procedemos al entrenamiento del modelo. Entrenaremos duarante 50 épocas y una paciencia de 5 para early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "modelo = MLPClassifier(input_dim=ccmusic_train.X.shape[1], num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos representar la red neuronal antes de entrenar para observar su estructura y comprobar si es necesaria alguna modificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(\n",
      "  (linear_block): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio del entrenamiento\n",
      "Época 1 Loss: 0.08833938091993332\n",
      "Época 2 Loss: 0.02928599715232849\n",
      "Época 3 Loss: 0.016496926546096802\n",
      "Época 4 Loss: 0.009050291031599045\n",
      "Época 5 Loss: 0.00845313910394907\n",
      "Época 6 Loss: 0.008805051445960999\n",
      "Época 7 Loss: 0.0062797898426651955\n",
      "Época 8 Loss: 0.005658080335706472\n",
      "Época 9 Loss: 0.0030308288987725973\n",
      "Época 10 Loss: 0.004965712316334248\n",
      "Época 11 Loss: 0.003865364706143737\n",
      "Época 12 Loss: 0.0034403956960886717\n",
      "Época 13 Loss: 0.0019635818898677826\n",
      "Época 14 Loss: 0.002340928418561816\n",
      "Época 15 Loss: 0.001330442144535482\n",
      "Época 16 Loss: 0.0012262616073712707\n",
      "Época 17 Loss: 0.0015109622618183494\n",
      "Época 18 Loss: 0.0005528116598725319\n",
      "Época 19 Loss: 0.0010406887158751488\n",
      "Época 20 Loss: 0.0004564994596876204\n",
      "Época 21 Loss: 0.00037578705814667046\n",
      "Época 22 Loss: 0.0003343092685099691\n",
      "Época 23 Loss: 0.00034876231802627444\n",
      "Época 24 Loss: 0.0002993019297719002\n",
      "Época 25 Loss: 0.00022636445646639913\n",
      "Época 26 Loss: 0.00020564223814290017\n",
      "Época 27 Loss: 0.00019710602646227926\n",
      "Época 28 Loss: 0.0001651683123782277\n",
      "Época 29 Loss: 0.00016556863556616008\n",
      "Época 30 Loss: 0.00012702427920885384\n",
      "Época 31 Loss: 0.00010536654735915363\n",
      "Época 32 Loss: 8.8655055151321e-05\n",
      "Época 33 Loss: 9.188220428768545e-05\n",
      "Época 34 Loss: 9.820584818953648e-05\n",
      "Época 35 Loss: 6.837725231889635e-05\n",
      "Época 36 Loss: 5.36668740096502e-05\n",
      "Época 37 Loss: 5.44791437278036e-05\n",
      "Época 38 Loss: 4.4089894799981266e-05\n",
      "Época 39 Loss: 4.8488200263818726e-05\n",
      "Época 40 Loss: 4.373216870590113e-05\n",
      "Época 41 Loss: 4.168458326603286e-05\n",
      "Época 42 Loss: 3.864007521769963e-05\n",
      "Época 43 Loss: 3.209612623322755e-05\n",
      "Época 44 Loss: 2.9560760594904423e-05\n",
      "Época 45 Loss: 3.101257243542932e-05\n",
      "Época 46 Loss: 2.662439874256961e-05\n",
      "Época 47 Loss: 2.3861421141191386e-05\n",
      "Época 48 Loss: 2.521582064218819e-05\n",
      "Época 49 Loss: 1.9938986952183768e-05\n",
      "Época 50 Loss: 1.7858526916825213e-05\n",
      "Fin del entrenamiento\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(modelo.parameters(), \n",
    "                             lr=LEARNING_RATE)\n",
    "train(modelo, ccmusic_train_dataloader, ccmusic_validation_dataloader, loss_fn, optimizer, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez entrenado evaluamos sobre el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en el conjunto de test: 0.9651162790697675\n",
      "F1 en el conjunto de test: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "metrics_test = evaluate_model(modelo, ccmusic_test_dataloader, num_classes=2)\n",
    "print(f\"Accuracy en el conjunto de test: {metrics_test['acc']}\")\n",
    "print(f\"F1 en el conjunto de test: {metrics_test['f1']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que las métricas sobre test son muy buenas. Estos nos indica que, efectivamente, las características obtenidas en los pasos anteriores identifican a cada uno de los dos géneros musicales de forma efectiva. Además, cabe destacar, que estos resultados se obtienen con un conjunto de datos y modelo simples, lo que permite su entrenamiento en cualquier máquina sin ningún requisito específico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
