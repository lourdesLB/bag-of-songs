{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Distinguir géneros musicales utilizando modelos de aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Modelos de aprendizaje basados en extracción de características (*bag of songs*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Creación del corpus de datos tabulares de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "\n",
    "\n",
    "class TabularDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, features_file, scaler=None):\n",
    "\n",
    "        df = pd.read_csv(features_file)\n",
    "\n",
    "        self.X = df.drop(['audio_file', 'label'], axis=1)\n",
    "        self.y = df['label'].values.astype(np.int64)\n",
    "\n",
    "        if scaler:\n",
    "            self.X = scaler.transform(self.X)\n",
    "        else:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.X = self.scaler.fit_transform(self.X)\n",
    "\n",
    "    def get_scaler(self):\n",
    "        return self.scaler\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.tolist()\n",
    "    \n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccmusic_train = TabularDataset('ccmusic/train/features.csv')\n",
    "train_scaler = ccmusic_train.get_scaler()\n",
    "ccmusic_train_dataloader = torch.utils.data.DataLoader(ccmusic_train, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       shuffle=True)\n",
    "\n",
    "ccmusic_validation = TabularDataset('ccmusic/validation/features.csv', train_scaler)\n",
    "ccmusic_validation_dataloader = torch.utils.data.DataLoader(ccmusic_validation, \n",
    "                                                            batch_size=BATCH_SIZE, \n",
    "                                                            shuffle=False)\n",
    "\n",
    "ccmusic_test = TabularDataset('ccmusic/test/features.csv', train_scaler)\n",
    "ccmusic_test_dataloader = torch.utils.data.DataLoader(ccmusic_test, \n",
    "                                                      batch_size=BATCH_SIZE, \n",
    "                                                      shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccmusic2_train = TabularDataset('ccmusic2/train/features.csv')\n",
    "train_scaler2 = ccmusic2_train.get_scaler()\n",
    "ccmusic2_train_dataloader = torch.utils.data.DataLoader(ccmusic2_train, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       shuffle=True)\n",
    "\n",
    "ccmusic2_validation = TabularDataset('ccmusic2/validation/features.csv', train_scaler2)\n",
    "ccmusic2_validation_dataloader = torch.utils.data.DataLoader(ccmusic2_validation, \n",
    "                                                            batch_size=BATCH_SIZE, \n",
    "                                                            shuffle=False)\n",
    "\n",
    "ccmusic2_test = TabularDataset('ccmusic2/test/features.csv', train_scaler2)\n",
    "ccmusic2_test_dataloader = torch.utils.data.DataLoader(ccmusic2_test, \n",
    "                                                      batch_size=BATCH_SIZE, \n",
    "                                                      shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Definición del modelo para la clasificación de generos en base a las características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "\n",
    "        self.linear_block = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            # torch.nn.BatchNorm1d(128),\n",
    "            # torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            # torch.nn.BatchNorm1d(64),\n",
    "            torch.nn.Linear(64, num_classes if num_classes > 2 else 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_epoch(model, train_dataloader, val_dataloader, loss_fn, optimizer):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for inputs, targets in train_dataloader:\n",
    "        print('.', end='')\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(inputs)\n",
    "        if isinstance(loss_fn, torch.nn.BCEWithLogitsLoss):\n",
    "            loss = loss_fn(predictions, targets.float().unsqueeze(1))\n",
    "        elif isinstance(loss_fn, torch.nn.CrossEntropyLoss):\n",
    "            loss = loss_fn(predictions, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_dataloader:\n",
    "            predictions = model(inputs)\n",
    "            if isinstance(loss_fn, torch.nn.BCEWithLogitsLoss):\n",
    "                loss = loss_fn(predictions, targets.float().unsqueeze(1))\n",
    "            elif isinstance(loss_fn, torch.nn.CrossEntropyLoss):\n",
    "                loss = loss_fn(predictions, targets)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_dataloader)\n",
    "\n",
    "    return loss.item(), val_loss\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, loss_fn, optimizer, epochs, patience=5):\n",
    "\n",
    "    print(\"Inicio del entrenamiento\")\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Época {epoch+1} \", end='')\n",
    "        loss, val_loss = train_single_epoch(model, train_dataloader, val_dataloader, loss_fn, optimizer)\n",
    "        print(f\"Loss: {loss}\")\n",
    "\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Deteniendo entrenamiento en la época {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    print(\"Fin del entrenamiento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, num_classes):\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        for inputs, target in dataloader:\n",
    "            output = model(inputs)\n",
    "            if num_classes > 2:\n",
    "                output = torch.argmax(output, dim=1)\n",
    "            else:\n",
    "                output = torch.sigmoid(output)\n",
    "                output = (output > 0.5)\n",
    "            predictions.append(output)\n",
    "            targets.append(target)\n",
    "        predictions = torch.cat(predictions, dim=0)\n",
    "        targets = torch.cat(targets, dim=0)\n",
    "        return {\n",
    "            'acc': accuracy_score(targets, predictions),\n",
    "            'f1': f1_score(targets, predictions) if num_classes == 2 \n",
    "            else f1_score(targets, predictions, average='micro')\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Entrenamiento e inferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CCMUSIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio del entrenamiento\n",
      "Época 1 ...........................................Loss: 0.12050341814756393\n",
      "Época 2 ...........................................Loss: 0.03188486769795418\n",
      "Época 3 ...........................................Loss: 0.021222978830337524\n",
      "Época 4 ...........................................Loss: 0.02205251157283783\n",
      "Época 5 ...........................................Loss: 0.008797035552561283\n",
      "Época 6 ...........................................Loss: 0.014521676115691662\n",
      "Época 7 ...........................................Loss: 0.008724812418222427\n",
      "Época 8 ...........................................Loss: 0.006520235911011696\n",
      "Época 9 ...........................................Loss: 0.006704649422317743\n",
      "Época 10 ...........................................Loss: 0.00487254885956645\n",
      "Época 11 ...........................................Loss: 0.005166085436940193\n",
      "Época 12 ...........................................Loss: 0.003538284217938781\n",
      "Época 13 ...........................................Loss: 0.003898274153470993\n",
      "Época 14 ...........................................Loss: 0.003384363604709506\n",
      "Época 15 ...........................................Loss: 0.0023331560660153627\n",
      "Época 16 ...........................................Loss: 0.001704324735328555\n",
      "Época 17 ...........................................Loss: 0.002316454192623496\n",
      "Época 18 ...........................................Loss: 0.0013628628803417087\n",
      "Época 19 ...........................................Loss: 0.0010448084212839603\n",
      "Época 20 ...........................................Loss: 0.0008280343026854098\n",
      "Época 21 ...........................................Loss: 0.001345493015833199\n",
      "Época 22 ...........................................Loss: 0.000858519459143281\n",
      "Época 23 ...........................................Loss: 0.0006935439887456596\n",
      "Época 24 ...........................................Loss: 0.0005913979257456958\n",
      "Época 25 ...........................................Loss: 0.00036231669946573675\n",
      "Época 26 ...........................................Loss: 0.00028700640541501343\n",
      "Época 27 ...........................................Loss: 0.00037646133569069207\n",
      "Época 28 ...........................................Loss: 0.0002630393428262323\n",
      "Época 29 ...........................................Loss: 0.00023446045815944672\n",
      "Época 30 ...........................................Loss: 0.0002433854533592239\n",
      "Época 31 ...........................................Loss: 0.0001573727058712393\n",
      "Época 32 ...........................................Loss: 0.00018956656276714057\n",
      "Época 33 ...........................................Loss: 0.00015140821051318198\n",
      "Época 34 ...........................................Loss: 0.00013119372306391597\n",
      "Época 35 ...........................................Loss: 0.00010576641943771392\n",
      "Época 36 ...........................................Loss: 0.00011371498112566769\n",
      "Época 37 ...........................................Loss: 0.00011249060480622575\n",
      "Época 38 ...........................................Loss: 8.344475645571947e-05\n",
      "Época 39 ...........................................Loss: 7.084750541253015e-05\n",
      "Época 40 ...........................................Loss: 6.478139403043315e-05\n",
      "Época 41 ...........................................Loss: 6.688282155664638e-05\n",
      "Época 42 ...........................................Loss: 5.596347546088509e-05\n",
      "Época 43 ...........................................Loss: 4.9268339353147894e-05\n",
      "Época 44 ...........................................Loss: 5.708983735530637e-05\n",
      "Época 45 ...........................................Loss: 4.9311671318719164e-05\n",
      "Época 46 ...........................................Loss: 4.176055881544016e-05\n",
      "Época 47 ...........................................Loss: 3.8391091948142275e-05\n",
      "Época 48 ...........................................Loss: 3.599665797082707e-05\n",
      "Época 49 ...........................................Loss: 3.788179310504347e-05\n",
      "Época 50 ...........................................Loss: 2.82498021988431e-05\n",
      "Fin del entrenamiento\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "modelo = MLPClassifier(input_dim=ccmusic_train.X.shape[1], num_classes=2)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(modelo.parameters(), \n",
    "                             lr=LEARNING_RATE)\n",
    "train(modelo, ccmusic_train_dataloader, ccmusic_validation_dataloader, loss_fn, optimizer, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en el conjunto de test: 0.9709302325581395\n",
      "F1 en el conjunto de test: 0.9816849816849816\n"
     ]
    }
   ],
   "source": [
    "metrics_test = evaluate_model(modelo, ccmusic_test_dataloader, num_classes=2)\n",
    "print(f\"Accuracy en el conjunto de test: {metrics_test['acc']}\")\n",
    "print(f\"F1 en el conjunto de test: {metrics_test['f1']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CCMUSIC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calcular class_weight \n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import sklearn.utils.class_weight\n",
    "\n",
    "# # Calcula los pesos de clase usando scikit-learn\n",
    "# class_weights = sklearn.utils.class_weight.compute_class_weight('balanced',\n",
    "#                                                                 np.unique(ccmusic2_train.y),\n",
    "#                                                                 ccmusic2_train.y)\n",
    "\n",
    "# # Convierte los pesos de clase a tensores de PyTorch\n",
    "# class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "# print(class_weights)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio del entrenamiento\n",
      "Época 1 ...........................................Loss: 3.6842195987701416\n",
      "Época 2 ...........................................Loss: 5.746178150177002\n",
      "Época 3 ...........................................Loss: 7.274298191070557\n",
      "Época 4 ."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................................Loss: 8.159215927124023\n",
      "Época 5 ...........................................Loss: 8.996831893920898\n",
      "Época 6 ...........................................Loss: 9.321578979492188\n",
      "Época 7 ...........................................Loss: 9.929741859436035\n",
      "Época 8 ...........................................Loss: 10.364989280700684\n",
      "Época 9 ...........................................Loss: 10.702692985534668\n",
      "Época 10 ...........................................Loss: 10.773736000061035\n",
      "Época 11 ...........................................Loss: 11.30105972290039\n",
      "Época 12 ...........................................Loss: 12.028177261352539\n",
      "Época 13 ...........................................Loss: 12.070975303649902\n",
      "Época 14 ...........................................Loss: 12.484526634216309\n",
      "Época 15 ...........................................Loss: 12.801424980163574\n",
      "Época 16 ...........................................Loss: 13.126769065856934\n",
      "Época 17 ...........................................Loss: 13.354828834533691\n",
      "Época 18 ...........................................Loss: 13.614557266235352\n",
      "Época 19 ...........................................Loss: 13.952775001525879\n",
      "Época 20 ...........................................Loss: 14.269832611083984\n",
      "Época 21 ...........................................Loss: 14.32772159576416\n",
      "Deteniendo entrenamiento en la época 21\n",
      "Fin del entrenamiento\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "modelo = MLPClassifier(input_dim=ccmusic2_train.X.shape[1], num_classes=9)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(modelo.parameters(), \n",
    "                             lr=LEARNING_RATE )\n",
    "train(modelo, ccmusic2_train_dataloader, ccmusic_validation_dataloader, loss_fn, optimizer, EPOCHS,\n",
    "      patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en el conjunto de test: 0.5523255813953488\n",
      "F1 en el conjunto de test: 0.5523255813953488\n"
     ]
    }
   ],
   "source": [
    "metrics_test = evaluate_model(modelo, ccmusic2_test_dataloader, num_classes=9)\n",
    "print(f\"Accuracy en el conjunto de test: {metrics_test['acc']}\")\n",
    "print(f\"F1 en el conjunto de test: {metrics_test['f1']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Modelos de aprendizaje basados en espectograma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Creación del corpus de espectogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase para la creación del corpus\n",
    "\n",
    "class SpectrogramDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, annotations_file, audios_dir, transformations=None):\n",
    "        super().__init__\n",
    "        self.annotations = self._leer_csv(annotations_file)\n",
    "        self.audios_dir = audios_dir\n",
    "        self.transformations = transformations\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        signal, sr = self._recupera_signal(index)\n",
    "        label = self._recupera_label(index)\n",
    "        if self.transformations:\n",
    "            signal = self.transformations(signal, sr)\n",
    "        return signal, label\n",
    "        \n",
    "    def mapa_label_classes(self):\n",
    "        pares=set([(id_label,label) for [_,id_label,label] in self.annotations])\n",
    "        mapa={}\n",
    "        for (a,b) in pares:\n",
    "            mapa[a]=b\n",
    "        return mapa\n",
    "    \n",
    "    def label_class(self, index):\n",
    "        return self.annotations[index][2]\n",
    "    \n",
    "    def get_audio_file(self, index): \n",
    "        name = self.annotations[index][0]\n",
    "        return os.path.join(name)\n",
    "\n",
    "    def _leer_csv(self,annotations_file):\n",
    "        with open(annotations_file, 'r', encoding='utf-8') as f:\n",
    "            lector = csv.reader(f)\n",
    "            next(lector) \n",
    "            data = [ (file_name, classID, classLabel)  \n",
    "                for file_name, classID, classLabel in lector]\n",
    "        return data\n",
    "\n",
    "    def _recupera_signal(self,index=0):\n",
    "        audio_file=self.get_audio_file(index)        \n",
    "        signal, sr = torchaudio.load(audio_file)\n",
    "        return signal, sr\n",
    "    \n",
    "    def _recupera_label(self, index):\n",
    "        return torch.tensor(int(self.annotations[index][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(signal, sr):\n",
    "    spectrogram_transform = torchaudio.transforms.Spectrogram(power=2)\n",
    "    spec_amplitud_to_db_transform = torchaudio.transforms.AmplitudeToDB()\n",
    "    spect = spectrogram_transform(signal)\n",
    "    spect=spec_amplitud_to_db_transform(spect)\n",
    "    return spect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccmusic_train = SpectrogramDataset(annotations_file='./ccmusic/train/annotations.csv', \n",
    "                                    audios_dir='./ccmusic/train/audios', \n",
    "                                    transformations=get_spectrogram)\n",
    "ccmusic_train_dataloader = torch.utils.data.DataLoader(ccmusic_train, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       shuffle=True)\n",
    "\n",
    "ccmusic_validation = SpectrogramDataset(annotations_file='./ccmusic/validation/annotations.csv',\n",
    "                                        audios_dir='./ccmusic/validation/audios',\n",
    "                                        transformations=get_spectrogram)\n",
    "ccmusic_validation_dataloader = torch.utils.data.DataLoader(ccmusic_validation, \n",
    "                                                            batch_size=BATCH_SIZE, \n",
    "                                                            shuffle=True)\n",
    "\n",
    "ccmusic_test = SpectrogramDataset(annotations_file='./ccmusic/test/annotations.csv',\n",
    "                                    audios_dir='./ccmusic/test/audios',\n",
    "                                    transformations=get_spectrogram)\n",
    "ccmusic_test_dataloader = torch.utils.data.DataLoader(ccmusic_test, \n",
    "                                                      batch_size=BATCH_SIZE, \n",
    "                                                      shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccmusic2_train = SpectrogramDataset(annotations_file='./ccmusic2/train/annotations.csv',\n",
    "                                    audios_dir='./ccmusic2/train/audios',\n",
    "                                    transformations=get_spectrogram)\n",
    "ccmusic2_train_dataloader = torch.utils.data.DataLoader(ccmusic2_train,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            shuffle=True)\n",
    "\n",
    "ccmusic2_validation = SpectrogramDataset(annotations_file='./ccmusic2/validation/annotations.csv',\n",
    "                                        audios_dir='./ccmusic2/validation/audios',\n",
    "                                        transformations=get_spectrogram)\n",
    "ccmusic2_validation_dataloader = torch.utils.data.DataLoader(ccmusic2_validation,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            shuffle=True)\n",
    "\n",
    "ccmusic2_test = SpectrogramDataset(annotations_file='./ccmusic2/test/annotations.csv',\n",
    "                                    audios_dir='./ccmusic2/test/audios',\n",
    "                                    transformations=get_spectrogram)\n",
    "ccmusic2_test_dataloader = torch.utils.data.DataLoader(ccmusic2_test,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 201, 3308])\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "espectograma, etiqueta = ccmusic_train[0]\n",
    "print(espectograma.shape)\n",
    "print(etiqueta)\n",
    "\n",
    "SPECTOGRAM_H = 201\n",
    "SPECTOGRAM_W = 3308"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Definición del modelo CNN para clasificación de espectogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcula_tam_capa_lineal1(spect_height, spect_width):\n",
    "\n",
    "    def pool_size(size, kernel_size, stride):\n",
    "        return (size - kernel_size) // stride + 1\n",
    "    \n",
    "    def conv_size(size, kernel_size, stride, padding):\n",
    "        return (size + 2*padding - kernel_size) // stride + 1\n",
    "\n",
    "    height_salida = spect_height\n",
    "    width_salida = spect_width\n",
    "\n",
    "    # Bloque convolucional: kernel_size=3, stride=1, padding=1\n",
    "\n",
    "    for _ in range(1, 4):\n",
    "        height_salida = pool_size(conv_size(height_salida, 3, 1, 1), 2, 2)\n",
    "        width_salida = pool_size(conv_size(width_salida, 3, 1, 1), 2, 2)\n",
    "\n",
    "    return height_salida * width_salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, spect_height, spect_width, num_labels):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        # Definir bloque convolucional\n",
    "        self.conv_block1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=32, \n",
    "                            kernel_size=3, \n",
    "                            stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.25),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = torch.nn.Sequential( \n",
    "            torch.nn.Conv2d(in_channels=32, out_channels=64,\n",
    "                            kernel_size=3, \n",
    "                            stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv_block3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=64, out_channels=128,\n",
    "                            kernel_size=3, \n",
    "                            stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        height_width_salida = calcula_tam_capa_lineal1(spect_height, spect_width)\n",
    "\n",
    "        self.mlp_block = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(1, -1),\n",
    "            torch.nn.Linear( 128 * height_width_salida, 512), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),  \n",
    "            torch.nn.Linear(512, num_labels if num_labels > 2 else 1),    \n",
    "            torch.nn.Softmax(dim=1) if num_labels > 2 else torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv_block1(input)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        predictions = self.mlp_block(x)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_epoch(model, dataloader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, targets in dataloader:\n",
    "        print('.', end='')\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_fn(predictions, targets.unsqueeze(1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return(loss.item())\n",
    "\n",
    "\n",
    "def train(model, dataloader, loss_fn, optimizer, epochs):\n",
    "    print(\"Inicio del entrenamiento\")\n",
    "    for i in range(epochs):\n",
    "        print(f\"Época {i+1} \" ,end='') \n",
    "        print('.', end='')\n",
    "        loss=train_single_epoch(model, dataloader, loss_fn, optimizer)\n",
    "        print(f\"\\nLoss: {loss}\")\n",
    "    print(\"Fin del entrenamiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3. Entrenamiento e inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = CNNModel(spect_height=SPECTOGRAM_H, spect_width=SPECTOGRAM_W, num_labels=2)\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(modelo.parameters(), \n",
    "                             lr=LEARNING_RATE)\n",
    "train(modelo, ccmusic_train_dataloader, loss_fn, optimizer, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = CNNModel(spect_height=SPECTOGRAM_H, spect_width=SPECTOGRAM_W, num_labels=9)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(modelo.parameters(), \n",
    "                             lr=LEARNING_RATE)\n",
    "train(modelo, ccmusic2_train_dataloader, loss_fn, optimizer, EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
