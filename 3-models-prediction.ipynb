{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Distinguir géneros musicales utilizando modelos de aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección nos centraremos en entrenar un modelo basado en las características obtenidas en el notebook anterior. Con estas características trataremos de identificar si cada una de las canciones pertenecen al género clásico o no clásico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Modelos de aprendizaje basados en extracción de características (*bag of songs*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Creación del corpus de datos tabulares de características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, haremos uso de torch para crear un dataset haciendo uso de las características de audio generadas anteiormente. El uso de esta estructura nos facilitará la posterior carga de datos en un DataLoader y su uso para el entrenamiento del modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, features_file, scaler=None):\n",
    "\n",
    "        df = pd.read_csv(features_file)\n",
    "\n",
    "        self.X = df.drop(['audio_file', 'label'], axis=1)\n",
    "        self.y = df['label'].values.astype(np.int64)\n",
    "\n",
    "        if scaler:\n",
    "            self.X = scaler.transform(self.X)\n",
    "        else:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.X = self.scaler.fit_transform(self.X)\n",
    "\n",
    "    def get_scaler(self):\n",
    "        return self.scaler\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.tolist()\n",
    "    \n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccmusic_train = TabularDataset('ccmusic/train/features.csv')\n",
    "train_scaler = ccmusic_train.get_scaler()\n",
    "ccmusic_train_dataloader = torch.utils.data.DataLoader(ccmusic_train, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       shuffle=True)\n",
    "\n",
    "ccmusic_validation = TabularDataset('ccmusic/validation/features.csv', train_scaler)\n",
    "ccmusic_validation_dataloader = torch.utils.data.DataLoader(ccmusic_validation, \n",
    "                                                            batch_size=BATCH_SIZE, \n",
    "                                                            shuffle=False)\n",
    "\n",
    "ccmusic_test = TabularDataset('ccmusic/test/features.csv', train_scaler)\n",
    "ccmusic_test_dataloader = torch.utils.data.DataLoader(ccmusic_test, \n",
    "                                                      batch_size=BATCH_SIZE, \n",
    "                                                      shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccmusic2_train = TabularDataset('ccmusic2/train/features.csv')\n",
    "train_scaler2 = ccmusic2_train.get_scaler()\n",
    "ccmusic2_train_dataloader = torch.utils.data.DataLoader(ccmusic2_train, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       shuffle=True)\n",
    "\n",
    "ccmusic2_validation = TabularDataset('ccmusic2/validation/features.csv', train_scaler2)\n",
    "ccmusic2_validation_dataloader = torch.utils.data.DataLoader(ccmusic2_validation, \n",
    "                                                            batch_size=BATCH_SIZE, \n",
    "                                                            shuffle=False)\n",
    "\n",
    "ccmusic2_test = TabularDataset('ccmusic2/test/features.csv', train_scaler2)\n",
    "ccmusic2_test_dataloader = torch.utils.data.DataLoader(ccmusic2_test, \n",
    "                                                      batch_size=BATCH_SIZE, \n",
    "                                                      shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Definición del modelo para la clasificación de generos en base a las características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cuanto a la elección del modelo, hemos optado por un modelo simple de perceptrón multicapa, con 3 capas lineales de 128, 64 y 1 neurona respectivamente. El restulado de esta última neurona nos indicará si la predicción corresponde a música clásica o no clásica. Como función de pérdida usaremos BinaryCrossEntropy. Con está función de loss no es necesario aplicar un función de activación sigmoide en la última capa del modelo. Además, haciendo uso del conjunto de validación, hemos aplicado la técnica de early stopping con una paciencia de 5. Con esto nos aseguramos que el modelo no se sobreajuste con los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "\n",
    "        self.linear_block = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            # torch.nn.BatchNorm1d(128),\n",
    "            # torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            # torch.nn.BatchNorm1d(64),\n",
    "            torch.nn.Linear(64, num_classes if num_classes > 2 else 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def train_single_epoch(model, train_dataloader, val_dataloader, loss_fn, optimizer, device):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for inputs, targets in train_dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(inputs)\n",
    "        if isinstance(loss_fn, torch.nn.BCEWithLogitsLoss):\n",
    "            loss = loss_fn(predictions, targets.float().unsqueeze(1))\n",
    "        elif isinstance(loss_fn, torch.nn.CrossEntropyLoss):\n",
    "            loss = loss_fn(predictions, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            predictions = model(inputs)\n",
    "            if isinstance(loss_fn, torch.nn.BCEWithLogitsLoss):\n",
    "                loss = loss_fn(predictions, targets.float().unsqueeze(1))\n",
    "            elif isinstance(loss_fn, torch.nn.CrossEntropyLoss):\n",
    "                loss = loss_fn(predictions, targets)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_dataloader)\n",
    "\n",
    "    return loss.item(), val_loss\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, loss_fn, optimizer, epochs, patience=5, device='cuda'):\n",
    "\n",
    "    model.to(device)\n",
    "    print(\"Inicio del entrenamiento\")\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Época {epoch+1} \", end='')\n",
    "        loss, val_loss = train_single_epoch(model, train_dataloader, val_dataloader, loss_fn, optimizer, device)\n",
    "        print(f\"Loss: {loss}\")\n",
    "\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Deteniendo entrenamiento en la época {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    print(\"Fin del entrenamiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez entrenado el modelo, pasamos a evaluar los resultados del conjunto de test, para determinar la bondad del modelo. Debido a que el dataset se encuentra desbalanceado, calcularemos las méticas de accuracy y F1, lo que nos indicará si realmente el modelo está realizando buenas predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, num_classes, device='cuda'):\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        for inputs, target in dataloader:\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            output = model(inputs)\n",
    "            if num_classes > 2:\n",
    "                output = torch.argmax(output, dim=1)\n",
    "            else:\n",
    "                output = torch.sigmoid(output)\n",
    "                output = (output > 0.5)\n",
    "            predictions.append(output)\n",
    "            targets.append(target)\n",
    "        predictions = torch.cat(predictions, dim=0)\n",
    "        targets = torch.cat(targets, dim=0)\n",
    "        return {\n",
    "            'acc': accuracy_score(targets.cpu(), predictions.cpu()),\n",
    "            'f1': f1_score(targets.cpu(), predictions.cpu()) if num_classes == 2 \n",
    "            else f1_score(targets.cpu(), predictions.cpu(), average='micro')\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Entrenamiento e inferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CCMUSIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez definidas las funciones necesarias procedemos al entrenamiento del modelo. Entrenaremos duarante 50 épocas y una paciencia de 5 para early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "modelo = MLPClassifier(input_dim=ccmusic_train.X.shape[1], num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos representar la red neuronal antes de entrenar para observar su estructura y comprobar si es necesaria alguna modificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netron\n",
    "\n",
    "# Guardar el estado del modelo\n",
    "torch.save(modelo.state_dict(), 'res/modelo_2tandas_weights.pth')\n",
    "\n",
    "# Exportar a ONNX\n",
    "torch.onnx.export(modelo,               # modelo instanciado\n",
    "                  'models/modelo.onnx',         # donde guardar el archivo ONNX resultante\n",
    "                  input_names=['input1', 'input2'],     # nombres de entrada\n",
    "                  output_names=['output']               # nombres de salida\n",
    ")\n",
    "# Lanzar visualización\n",
    "netron.start('models/modelo.onnx', browse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(modelo.parameters(), \n",
    "                             lr=LEARNING_RATE)\n",
    "train(modelo, ccmusic_train_dataloader, ccmusic_validation_dataloader, loss_fn, optimizer, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez entrenado evaluamos sobre el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en el conjunto de test: 0.9709302325581395\n",
      "F1 en el conjunto de test: 0.981549815498155\n"
     ]
    }
   ],
   "source": [
    "metrics_test = evaluate_model(modelo, ccmusic_test_dataloader, num_classes=2)\n",
    "print(f\"Accuracy en el conjunto de test: {metrics_test['acc']}\")\n",
    "print(f\"F1 en el conjunto de test: {metrics_test['f1']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que las métricas sobre test son muy buenas. Estos nos indica que, efectivamente, las características obtenidas en los pasos anteriores identifican a cada uno de los dos géneros musicales de forma efectiva. Además, cabe destacar, que estos resultados se obtienen con un conjunto de datos y modelo simples, lo que permite su entrenamiento en cualquier máquina sin ningún requisito específico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CCMUSIC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calcular class_weight \n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import sklearn.utils.class_weight\n",
    "\n",
    "# # Calcula los pesos de clase usando scikit-learn\n",
    "# class_weights = sklearn.utils.class_weight.compute_class_weight('balanced',\n",
    "#                                                                 np.unique(ccmusic2_train.y),\n",
    "#                                                                 ccmusic2_train.y)\n",
    "\n",
    "# # Convierte los pesos de clase a tensores de PyTorch\n",
    "# class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "# print(class_weights)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio del entrenamiento\n",
      "Época 1 Loss: 3.721733808517456\n",
      "Época 2 Loss: 5.6760640144348145\n",
      "Época 3 Loss: 6.745384693145752\n",
      "Época 4 Loss: 7.6535258293151855\n",
      "Época 5 Loss: 8.097639083862305\n",
      "Época 6 Loss: 8.481220245361328\n",
      "Época 7 Loss: 8.62679672241211\n",
      "Época 8 Loss: 9.123586654663086\n",
      "Época 9 Loss: 9.292040824890137\n",
      "Época 10 Loss: 9.637316703796387\n",
      "Época 11 Loss: 9.899700164794922\n",
      "Época 12 Loss: 10.337506294250488\n",
      "Época 13 Loss: 10.493694305419922\n",
      "Época 14 Loss: 10.646076202392578\n",
      "Época 15 Loss: 10.925032615661621\n",
      "Época 16 Loss: 10.982606887817383\n",
      "Época 17 Loss: 11.610930442810059\n",
      "Época 18 Loss: 11.708135604858398\n",
      "Época 19 Loss: 11.9278564453125\n",
      "Época 20 Loss: 11.982645034790039\n",
      "Época 21 Loss: 12.389301300048828\n",
      "Deteniendo entrenamiento en la época 21\n",
      "Fin del entrenamiento\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "modelo = MLPClassifier(input_dim=ccmusic2_train.X.shape[1], num_classes=9)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(modelo.parameters(), \n",
    "                             lr=LEARNING_RATE )\n",
    "train(modelo, ccmusic2_train_dataloader, ccmusic_validation_dataloader, loss_fn, optimizer, EPOCHS,\n",
    "      patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en el conjunto de test: 0.5523255813953488\n",
      "F1 en el conjunto de test: 0.5523255813953488\n"
     ]
    }
   ],
   "source": [
    "metrics_test = evaluate_model(modelo, ccmusic2_test_dataloader, num_classes=9)\n",
    "print(f\"Accuracy en el conjunto de test: {metrics_test['acc']}\")\n",
    "print(f\"F1 en el conjunto de test: {metrics_test['f1']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
