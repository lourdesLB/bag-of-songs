{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Distinguir géneros musicales utilizando modelos de aprendizaje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import focal_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Modelos de aprendizaje basados en extracción de características (*bag of songs*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Creación del corpus de datos tabulares de características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "\n",
    "\n",
    "class TabularDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, features_file, scaler=None):\n",
    "\n",
    "        df = pd.read_csv(features_file)\n",
    "\n",
    "        self.X = df.drop(['audio_file', 'label'], axis=1)\n",
    "        self.y = df['label'].values.astype(np.int64)\n",
    "\n",
    "        if scaler:\n",
    "            self.X = scaler.transform(self.X)\n",
    "        else:\n",
    "            self.scaler = StandardScaler()\n",
    "            self.X = self.scaler.fit_transform(self.X)\n",
    "\n",
    "    def get_scaler(self):\n",
    "        return self.scaler\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            idx = idx.tolist()\n",
    "    \n",
    "        return torch.tensor(self.X[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccmusic_train = TabularDataset('ccmusic/train/features.csv')\n",
    "train_scaler = ccmusic_train.get_scaler()\n",
    "ccmusic_train_dataloader = torch.utils.data.DataLoader(ccmusic_train, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       shuffle=True)\n",
    "\n",
    "ccmusic_validation = TabularDataset('ccmusic/validation/features.csv', train_scaler)\n",
    "ccmusic_validation_dataloader = torch.utils.data.DataLoader(ccmusic_validation, \n",
    "                                                            batch_size=BATCH_SIZE, \n",
    "                                                            shuffle=False)\n",
    "\n",
    "ccmusic_test = TabularDataset('ccmusic/test/features.csv', train_scaler)\n",
    "ccmusic_test_dataloader = torch.utils.data.DataLoader(ccmusic_test, \n",
    "                                                      batch_size=BATCH_SIZE, \n",
    "                                                      shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccmusic2_train = TabularDataset('ccmusic2/train/features.csv')\n",
    "train_scaler2 = ccmusic2_train.get_scaler()\n",
    "ccmusic2_train_dataloader = torch.utils.data.DataLoader(ccmusic2_train, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       shuffle=True)\n",
    "\n",
    "ccmusic2_validation = TabularDataset('ccmusic2/validation/features.csv', train_scaler2)\n",
    "ccmusic2_validation_dataloader = torch.utils.data.DataLoader(ccmusic2_validation, \n",
    "                                                            batch_size=BATCH_SIZE, \n",
    "                                                            shuffle=False)\n",
    "\n",
    "ccmusic2_test = TabularDataset('ccmusic2/test/features.csv', train_scaler2)\n",
    "ccmusic2_test_dataloader = torch.utils.data.DataLoader(ccmusic2_test, \n",
    "                                                      batch_size=BATCH_SIZE, \n",
    "                                                      shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Definición del modelo para la clasificación de generos en base a las características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "\n",
    "        self.linear_block = torch.nn.Sequential(\n",
    "            torch.nn.Linear(input_dim, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            # torch.nn.BatchNorm1d(128),\n",
    "            # torch.nn.Dropout(0.5),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            # torch.nn.BatchNorm1d(64),\n",
    "            torch.nn.Linear(64, num_classes if num_classes > 2 else 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def train_single_epoch(model, train_dataloader, val_dataloader, loss_fn, optimizer, device):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for inputs, targets in train_dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(inputs)\n",
    "        if isinstance(loss_fn, torch.nn.BCEWithLogitsLoss):\n",
    "            loss = loss_fn(predictions, targets.float().unsqueeze(1))\n",
    "        elif isinstance(loss_fn, torch.nn.CrossEntropyLoss):\n",
    "            loss = loss_fn(predictions, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            predictions = model(inputs)\n",
    "            if isinstance(loss_fn, torch.nn.BCEWithLogitsLoss):\n",
    "                loss = loss_fn(predictions, targets.float().unsqueeze(1))\n",
    "            elif isinstance(loss_fn, torch.nn.CrossEntropyLoss):\n",
    "                loss = loss_fn(predictions, targets)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_dataloader)\n",
    "\n",
    "    return loss.item(), val_loss\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, loss_fn, optimizer, epochs, patience=5, device='cuda'):\n",
    "\n",
    "    model.to(device)\n",
    "    print(\"Inicio del entrenamiento\")\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Época {epoch+1} \", end='')\n",
    "        loss, val_loss = train_single_epoch(model, train_dataloader, val_dataloader, loss_fn, optimizer, device)\n",
    "        print(f\"Loss: {loss}\")\n",
    "\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Deteniendo entrenamiento en la época {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    print(\"Fin del entrenamiento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, num_classes, device='cuda'):\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        for inputs, target in dataloader:\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            output = model(inputs)\n",
    "            if num_classes > 2:\n",
    "                output = torch.argmax(output, dim=1)\n",
    "            else:\n",
    "                output = torch.sigmoid(output)\n",
    "                output = (output > 0.5)\n",
    "            predictions.append(output)\n",
    "            targets.append(target)\n",
    "        predictions = torch.cat(predictions, dim=0)\n",
    "        targets = torch.cat(targets, dim=0)\n",
    "        return {\n",
    "            'acc': accuracy_score(targets.cpu(), predictions.cpu()),\n",
    "            'f1': f1_score(targets.cpu(), predictions.cpu()) if num_classes == 2 \n",
    "            else f1_score(targets.cpu(), predictions.cpu(), average='micro')\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Entrenamiento e inferencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CCMUSIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio del entrenamiento\n",
      "Época 1 Loss: 0.1144166886806488\n",
      "Época 2 Loss: 0.03076895885169506\n",
      "Época 3 Loss: 0.021174129098653793\n",
      "Época 4 Loss: 0.01606043241918087\n",
      "Época 5 Loss: 0.012176807038486004\n",
      "Época 6 Loss: 0.010602782480418682\n",
      "Época 7 Loss: 0.006224792916327715\n",
      "Época 8 Loss: 0.006463612429797649\n",
      "Época 9 Loss: 0.0033868453465402126\n",
      "Época 10 Loss: 0.002547886222600937\n",
      "Época 11 Loss: 0.005453024059534073\n",
      "Época 12 Loss: 0.0018593418644741178\n",
      "Época 13 Loss: 0.002986837876960635\n",
      "Época 14 Loss: 0.003007487626746297\n",
      "Época 15 Loss: 0.0013803510228171945\n",
      "Época 16 Loss: 0.0014926667790859938\n",
      "Época 17 Loss: 0.0010029763216152787\n",
      "Época 18 Loss: 0.0008025760762393475\n",
      "Época 19 Loss: 0.0005203281762078404\n",
      "Época 20 Loss: 0.0005686854710802436\n",
      "Época 21 Loss: 0.0003789956390392035\n",
      "Época 22 Loss: 0.0005032019107602537\n",
      "Época 23 Loss: 0.0003781775594688952\n",
      "Época 24 Loss: 0.00039017703966237605\n",
      "Época 25 Loss: 0.0002719030308071524\n",
      "Época 26 Loss: 0.0001608995662536472\n",
      "Época 27 Loss: 0.00010851704428205267\n",
      "Época 28 Loss: 0.00011906206054845825\n",
      "Época 29 Loss: 0.00015949390945024788\n",
      "Época 30 Loss: 8.346747927134857e-05\n",
      "Época 31 Loss: 9.987279190681875e-05\n",
      "Época 32 Loss: 6.251209561014548e-05\n",
      "Época 33 Loss: 6.299486994976178e-05\n",
      "Época 34 Loss: 4.786147110280581e-05\n",
      "Época 35 Loss: 4.424392318469472e-05\n",
      "Época 36 Loss: 3.602752258302644e-05\n",
      "Época 37 Loss: 3.450042277108878e-05\n",
      "Época 38 Loss: 2.700998811633326e-05\n",
      "Época 39 Loss: 2.9371898563113064e-05\n",
      "Época 40 Loss: 3.098756496910937e-05\n",
      "Época 41 Loss: 1.8227228792966343e-05\n",
      "Época 42 Loss: 2.0881087039015256e-05\n",
      "Época 43 Loss: 2.0159646737738512e-05\n",
      "Época 44 Loss: 1.8408603864372708e-05\n",
      "Época 45 Loss: 1.5184678886726033e-05\n",
      "Época 46 Loss: 1.265509763470618e-05\n",
      "Época 47 Loss: 1.3419824426819105e-05\n",
      "Época 48 Loss: 1.0370707059337292e-05\n",
      "Época 49 Loss: 8.456793693767395e-06\n",
      "Época 50 Loss: 9.396180757903494e-06\n",
      "Fin del entrenamiento\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 50\n",
    "modelo = MLPClassifier(input_dim=ccmusic_train.X.shape[1], num_classes=2)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(modelo.parameters(), \n",
    "                             lr=LEARNING_RATE)\n",
    "train(modelo, ccmusic_train_dataloader, ccmusic_validation_dataloader, loss_fn, optimizer, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en el conjunto de test: 0.9709302325581395\n",
      "F1 en el conjunto de test: 0.981549815498155\n"
     ]
    }
   ],
   "source": [
    "metrics_test = evaluate_model(modelo, ccmusic_test_dataloader, num_classes=2)\n",
    "print(f\"Accuracy en el conjunto de test: {metrics_test['acc']}\")\n",
    "print(f\"F1 en el conjunto de test: {metrics_test['f1']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CCMUSIC2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calcular class_weight \n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import sklearn.utils.class_weight\n",
    "\n",
    "# # Calcula los pesos de clase usando scikit-learn\n",
    "# class_weights = sklearn.utils.class_weight.compute_class_weight('balanced',\n",
    "#                                                                 np.unique(ccmusic2_train.y),\n",
    "#                                                                 ccmusic2_train.y)\n",
    "\n",
    "# # Convierte los pesos de clase a tensores de PyTorch\n",
    "# class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "\n",
    "# print(class_weights)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio del entrenamiento\n",
      "Época 1 Loss: 3.721733808517456\n",
      "Época 2 Loss: 5.6760640144348145\n",
      "Época 3 Loss: 6.745384693145752\n",
      "Época 4 Loss: 7.6535258293151855\n",
      "Época 5 Loss: 8.097639083862305\n",
      "Época 6 Loss: 8.481220245361328\n",
      "Época 7 Loss: 8.62679672241211\n",
      "Época 8 Loss: 9.123586654663086\n",
      "Época 9 Loss: 9.292040824890137\n",
      "Época 10 Loss: 9.637316703796387\n",
      "Época 11 Loss: 9.899700164794922\n",
      "Época 12 Loss: 10.337506294250488\n",
      "Época 13 Loss: 10.493694305419922\n",
      "Época 14 Loss: 10.646076202392578\n",
      "Época 15 Loss: 10.925032615661621\n",
      "Época 16 Loss: 10.982606887817383\n",
      "Época 17 Loss: 11.610930442810059\n",
      "Época 18 Loss: 11.708135604858398\n",
      "Época 19 Loss: 11.9278564453125\n",
      "Época 20 Loss: 11.982645034790039\n",
      "Época 21 Loss: 12.389301300048828\n",
      "Deteniendo entrenamiento en la época 21\n",
      "Fin del entrenamiento\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "modelo = MLPClassifier(input_dim=ccmusic2_train.X.shape[1], num_classes=9)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(modelo.parameters(), \n",
    "                             lr=LEARNING_RATE )\n",
    "train(modelo, ccmusic2_train_dataloader, ccmusic_validation_dataloader, loss_fn, optimizer, EPOCHS,\n",
    "      patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en el conjunto de test: 0.5523255813953488\n",
      "F1 en el conjunto de test: 0.5523255813953488\n"
     ]
    }
   ],
   "source": [
    "metrics_test = evaluate_model(modelo, ccmusic2_test_dataloader, num_classes=9)\n",
    "print(f\"Accuracy en el conjunto de test: {metrics_test['acc']}\")\n",
    "print(f\"F1 en el conjunto de test: {metrics_test['f1']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Modelos de aprendizaje basados en espectograma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. Creación del corpus de espectogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase para la creación del corpus\n",
    "\n",
    "class SpectrogramDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, annotations_file, audios_dir, transformations=None):\n",
    "        super().__init__\n",
    "        self.annotations = self._leer_csv(annotations_file)\n",
    "        self.audios_dir = audios_dir\n",
    "        self.transformations = transformations\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        signal, sr = self._recupera_signal(index)\n",
    "        label = self._recupera_label(index)\n",
    "        if self.transformations:\n",
    "            signal = self.transformations(signal, sr)\n",
    "        return signal, label\n",
    "        \n",
    "    def mapa_label_classes(self):\n",
    "        pares=set([(id_label,label) for [_,id_label,label] in self.annotations])\n",
    "        mapa={}\n",
    "        for (a,b) in pares:\n",
    "            mapa[a]=b\n",
    "        return mapa\n",
    "    \n",
    "    def label_class(self, index):\n",
    "        return self.annotations[index][2]\n",
    "    \n",
    "    def get_audio_file(self, index): \n",
    "        name = self.annotations[index][0]\n",
    "        return os.path.join(name)\n",
    "\n",
    "    def _leer_csv(self,annotations_file):\n",
    "        with open(annotations_file, 'r', encoding='utf-8') as f:\n",
    "            lector = csv.reader(f)\n",
    "            next(lector) \n",
    "            data = [ (file_name, classID, classLabel)  \n",
    "                for file_name, classID, classLabel in lector]\n",
    "        return data\n",
    "\n",
    "    def _recupera_signal(self,index=0):\n",
    "        audio_file=self.get_audio_file(index)        \n",
    "        signal, sr = torchaudio.load(audio_file)\n",
    "        return signal, sr\n",
    "    \n",
    "    def _recupera_label(self, index):\n",
    "        return torch.tensor(int(self.annotations[index][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spectrogram(signal, sr):\n",
    "    spectrogram_transform = torchaudio.transforms.Spectrogram(power=2)\n",
    "    spec_amplitud_to_db_transform = torchaudio.transforms.AmplitudeToDB()\n",
    "    spect = spectrogram_transform(signal)\n",
    "    spect=spec_amplitud_to_db_transform(spect)\n",
    "    return spect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccmusic_train = SpectrogramDataset(annotations_file='./ccmusic/train/annotations.csv', \n",
    "                                    audios_dir='./ccmusic/train/audios', \n",
    "                                    transformations=get_spectrogram)\n",
    "ccmusic_train_dataloader = torch.utils.data.DataLoader(ccmusic_train, \n",
    "                                                       batch_size=BATCH_SIZE, \n",
    "                                                       shuffle=True)\n",
    "\n",
    "ccmusic_validation = SpectrogramDataset(annotations_file='./ccmusic/validation/annotations.csv',\n",
    "                                        audios_dir='./ccmusic/validation/audios',\n",
    "                                        transformations=get_spectrogram)\n",
    "ccmusic_validation_dataloader = torch.utils.data.DataLoader(ccmusic_validation, \n",
    "                                                            batch_size=BATCH_SIZE, \n",
    "                                                            shuffle=True)\n",
    "\n",
    "ccmusic_test = SpectrogramDataset(annotations_file='./ccmusic/test/annotations.csv',\n",
    "                                    audios_dir='./ccmusic/test/audios',\n",
    "                                    transformations=get_spectrogram)\n",
    "ccmusic_test_dataloader = torch.utils.data.DataLoader(ccmusic_test, \n",
    "                                                      batch_size=BATCH_SIZE, \n",
    "                                                      shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccmusic2_train = SpectrogramDataset(annotations_file='./ccmusic2/train/annotations.csv',\n",
    "                                    audios_dir='./ccmusic2/train/audios',\n",
    "                                    transformations=get_spectrogram)\n",
    "ccmusic2_train_dataloader = torch.utils.data.DataLoader(ccmusic2_train,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            shuffle=True)\n",
    "\n",
    "ccmusic2_validation = SpectrogramDataset(annotations_file='./ccmusic2/validation/annotations.csv',\n",
    "                                        audios_dir='./ccmusic2/validation/audios',\n",
    "                                        transformations=get_spectrogram)\n",
    "ccmusic2_validation_dataloader = torch.utils.data.DataLoader(ccmusic2_validation,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            shuffle=True)\n",
    "\n",
    "ccmusic2_test = SpectrogramDataset(annotations_file='./ccmusic2/test/annotations.csv',\n",
    "                                    audios_dir='./ccmusic2/test/audios',\n",
    "                                    transformations=get_spectrogram)\n",
    "ccmusic2_test_dataloader = torch.utils.data.DataLoader(ccmusic2_test,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't find appropriate backend to handle uri ccmusic/train/audios/audio_train_0.wav and format None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m espectograma, etiqueta \u001b[38;5;241m=\u001b[39m \u001b[43mccmusic_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(espectograma\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(etiqueta)\n",
      "Cell \u001b[0;32mIn[29], line 15\u001b[0m, in \u001b[0;36mSpectrogramDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m---> 15\u001b[0m     signal, sr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recupera_signal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recupera_label(index)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformations:\n",
      "Cell \u001b[0;32mIn[29], line 45\u001b[0m, in \u001b[0;36mSpectrogramDataset._recupera_signal\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recupera_signal\u001b[39m(\u001b[38;5;28mself\u001b[39m,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     44\u001b[0m     audio_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_audio_file(index)        \n\u001b[0;32m---> 45\u001b[0m     signal, sr \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m signal, sr\n",
      "File \u001b[0;32m~/aine/.venv/lib/python3.10/site-packages/torchaudio/_backend/utils.py:204\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    119\u001b[0m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    120\u001b[0m     frame_offset: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    By default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     backend \u001b[38;5;241m=\u001b[39m \u001b[43mdispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mload(uri, frame_offset, num_frames, normalize, channels_first, \u001b[38;5;28mformat\u001b[39m, buffer_size)\n",
      "File \u001b[0;32m~/aine/.venv/lib/python3.10/site-packages/torchaudio/_backend/utils.py:116\u001b[0m, in \u001b[0;36mget_load_func.<locals>.dispatcher\u001b[0;34m(uri, format, backend_name)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcan_decode(uri, \u001b[38;5;28mformat\u001b[39m):\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m backend\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find appropriate backend to handle uri \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and format \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mformat\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Couldn't find appropriate backend to handle uri ccmusic/train/audios/audio_train_0.wav and format None."
     ]
    }
   ],
   "source": [
    "espectograma, etiqueta = ccmusic_train[0]\n",
    "print(espectograma.shape)\n",
    "print(etiqueta)\n",
    "\n",
    "SPECTOGRAM_H = 201\n",
    "SPECTOGRAM_W = 3308"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Definición del modelo CNN para clasificación de espectogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcula_tam_capa_lineal1(spect_height, spect_width):\n",
    "\n",
    "    def pool_size(size, kernel_size, stride):\n",
    "        return (size - kernel_size) // stride + 1\n",
    "    \n",
    "    def conv_size(size, kernel_size, stride, padding):\n",
    "        return (size + 2*padding - kernel_size) // stride + 1\n",
    "\n",
    "    height_salida = spect_height\n",
    "    width_salida = spect_width\n",
    "\n",
    "    # Bloque convolucional: kernel_size=3, stride=1, padding=1\n",
    "\n",
    "    for _ in range(1, 4):\n",
    "        height_salida = pool_size(conv_size(height_salida, 3, 1, 1), 2, 2)\n",
    "        width_salida = pool_size(conv_size(width_salida, 3, 1, 1), 2, 2)\n",
    "\n",
    "    return height_salida * width_salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, spect_height, spect_width, num_labels):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        # Definir bloque convolucional\n",
    "        self.conv_block1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=1, out_channels=32, \n",
    "                            kernel_size=3, \n",
    "                            stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.25),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = torch.nn.Sequential( \n",
    "            torch.nn.Conv2d(in_channels=32, out_channels=64,\n",
    "                            kernel_size=3, \n",
    "                            stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.conv_block3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels=64, out_channels=128,\n",
    "                            kernel_size=3, \n",
    "                            stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        height_width_salida = calcula_tam_capa_lineal1(spect_height, spect_width)\n",
    "\n",
    "        self.mlp_block = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(1, -1),\n",
    "            torch.nn.Linear( 128 * height_width_salida, 512), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),  \n",
    "            torch.nn.Linear(512, num_labels if num_labels > 2 else 1),    \n",
    "            torch.nn.Softmax(dim=1) if num_labels > 2 else torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv_block1(input)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.conv_block3(x)\n",
    "        predictions = self.mlp_block(x)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_epoch(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_fn(predictions, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def train(model, dataloader, loss_fn, optimizer, epochs, device=\"cuda\"):\n",
    "    print(\"Inicio del entrenamiento\")\n",
    "    for i in range(epochs):\n",
    "        print(f\"Época {i+1}\", end='')\n",
    "        loss = train_single_epoch(model, dataloader, loss_fn, optimizer, device)\n",
    "        print(f\"\\nLoss: {loss:.4f}\")\n",
    "    print(\"Fin del entrenamiento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3. Entrenamiento e inferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "modelo = CNNModel(spect_height=SPECTOGRAM_H, spect_width=SPECTOGRAM_W, num_labels=2)\n",
    "modelo.to(device)  # Mover el modelo al dispositivo apropiado\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(modelo.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train(modelo, ccmusic_train_dataloader, loss_fn, optimizer, EPOCHS, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device, num_classes):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    f1 = f1_score(all_targets, all_predictions, average='weighted')\n",
    "    return {'acc': accuracy, 'f1': f1}\n",
    "\n",
    "# Evaluar el modelo en el conjunto de test\n",
    "metrics_test = evaluate_model(modelo, ccmusic2_test_dataloader, device, num_classes=9)\n",
    "print(f\"Accuracy en el conjunto de test: {metrics_test['acc']:.2f}\")\n",
    "print(f\"F1 en el conjunto de test: {metrics_test['f1']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = CNNModel(spect_height=SPECTOGRAM_H, spect_width=SPECTOGRAM_W, num_labels=9)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(modelo.parameters(), \n",
    "                             lr=LEARNING_RATE)\n",
    "train(modelo, ccmusic2_train_dataloader, loss_fn, optimizer, EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
